{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bfh_my0erwbM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xwn5DoPbr9oj"
      },
      "outputs": [],
      "source": [
        "dataset_path = os.listdir('/home/mes2/Desktop/Video Classification/train')\n",
        "\n",
        "label_types = os.listdir('/home/mes2/Desktop/Video Classification/train')\n",
        "print (label_types)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EbqTLAVYuK1M"
      },
      "outputs": [],
      "source": [
        "rooms = []\n",
        "\n",
        "for item in dataset_path:\n",
        " # Get all the file names\n",
        " all_rooms = os.listdir('/home/mes2/Desktop/Video Classification/train' + '/' +item)\n",
        "\n",
        " # Add them to the list\n",
        " for room in all_rooms:\n",
        "    rooms.append((item, str('/home/mes2/Desktop/Video Classification/train' + '/' +item) + '/' + room))\n",
        "\n",
        "# Build a dataframe\n",
        "train_df = pd.DataFrame(data=rooms, columns=['tag', 'video_name'])\n",
        "print(train_df.head())\n",
        "print(train_df.tail())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LqVLFGtuiNh"
      },
      "outputs": [],
      "source": [
        "df = train_df.loc[:,['video_name','tag']]\n",
        "df\n",
        "df.to_csv('train.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PViOhhN8vPXI"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv('train.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QfLc-LXW2DKD"
      },
      "outputs": [],
      "source": [
        "dataset_path = os.listdir('/home/mes2/Desktop/Video Classification/test')\n",
        "print(dataset_path)\n",
        "\n",
        "room_types = os.listdir('/home/mes2/Desktop/Video Classification/test')\n",
        "print(\"Types of activities found: \", len(dataset_path))\n",
        "\n",
        "rooms = []\n",
        "\n",
        "for item in dataset_path:\n",
        " # Get all the file names\n",
        " all_rooms = os.listdir('/home/mes2/Desktop/Video Classification/test' + '/' +item)\n",
        "\n",
        " # Add them to the list\n",
        " for room in all_rooms:\n",
        "    rooms.append((item, str('/home/mes2/Desktop/Video Classification/test' + '/' +item) + '/' + room))\n",
        "\n",
        "# Build a dataframe\n",
        "test_df = pd.DataFrame(data=rooms, columns=['tag', 'video_name'])\n",
        "print(test_df.head())\n",
        "print(test_df.tail())\n",
        "\n",
        "df = test_df.loc[:,['video_name','tag']]\n",
        "df\n",
        "df.to_csv('test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7onL4HE2Xob"
      },
      "outputs": [],
      "source": [
        "test_df = pd.read_csv('test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQUjHYIF2ekO"
      },
      "outputs": [],
      "source": [
        "len(test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1VabkPaM4BU"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/tensorflow/docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qTjAJsEM4BV"
      },
      "outputs": [],
      "source": [
        "pip install --upgrade imutils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9c7vfBPCM4BV"
      },
      "outputs": [],
      "source": [
        "  pip install imageio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "flz3IjpQvbeR"
      },
      "outputs": [],
      "source": [
        "from tensorflow_docs.vis import embed\n",
        "from tensorflow import keras\n",
        "from imutils import paths\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import imageio\n",
        "import cv2\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwTeTKGUv0_y"
      },
      "outputs": [],
      "source": [
        "len(train_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxzobLyvwYay"
      },
      "outputs": [],
      "source": [
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "  try:\n",
        "    tf.config.experimental.set_virtual_device_configuration(\n",
        "        gpus[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=5120)])\n",
        "  except RuntimeError as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hE9wGfJ5wkg1"
      },
      "outputs": [],
      "source": [
        "IMG_SIZE = 224\n",
        "\n",
        "\n",
        "def crop_center_square(frame):\n",
        "    y, x = frame.shape[0:2]\n",
        "    min_dim = min(y, x)\n",
        "    start_x = (x // 2) - (min_dim // 2)\n",
        "    start_y = (y // 2) - (min_dim // 2)\n",
        "    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]\n",
        "\n",
        "\n",
        "def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    frames = []\n",
        "    try:\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frame = crop_center_square(frame)\n",
        "            frame = cv2.resize(frame, resize)\n",
        "            frame = frame[:, :, [2, 1, 0]]\n",
        "            frames.append(frame)\n",
        "\n",
        "            if len(frames) == max_frames:\n",
        "                break\n",
        "    finally:\n",
        "        cap.release()\n",
        "    return np.array(frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFEf5-SLwrek"
      },
      "outputs": [],
      "source": [
        "def build_feature_extractor():\n",
        "    feature_extractor = keras.applications.InceptionV3(\n",
        "        weights=\"imagenet\",\n",
        "        include_top=False,\n",
        "        pooling=\"avg\",\n",
        "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
        "    )\n",
        "    preprocess_input = keras.applications.inception_v3.preprocess_input\n",
        "\n",
        "    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
        "    preprocessed = preprocess_input(inputs)\n",
        "\n",
        "    outputs = feature_extractor(preprocessed)\n",
        "    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
        "\n",
        "\n",
        "feature_extractor = build_feature_extractor()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2i0SRssiw2Bu"
      },
      "outputs": [],
      "source": [
        "label_processor = keras.layers.StringLookup(num_oov_indices=0, vocabulary=np.unique(train_df[\"tag\"]))\n",
        "print(label_processor.get_vocabulary())\n",
        "\n",
        "labels = train_df[\"tag\"].values\n",
        "labels = label_processor(labels[..., None]).numpy()\n",
        "labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywxrLTQ4xEak"
      },
      "outputs": [],
      "source": [
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 100\n",
        "\n",
        "MAX_SEQ_LENGTH = 20\n",
        "NUM_FEATURES = 2048"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XkT2BkoyxOVD"
      },
      "outputs": [],
      "source": [
        "def prepare_all_videos(df, root_dir):\n",
        "    num_samples = len(df)\n",
        "    video_paths = df[\"video_name\"].values.tolist()\n",
        "\n",
        "    ##take all classlabels from train_df column named 'tag' and store in labels\n",
        "    labels = df[\"tag\"].values\n",
        "\n",
        "    #convert classlabels to label encoding\n",
        "    labels = label_processor(labels[..., None]).numpy()\n",
        "\n",
        "    # `frame_masks` and `frame_features` are what we will feed to our sequence model.\n",
        "    # `frame_masks` will contain a bunch of booleans denoting if a timestep is\n",
        "    # masked with padding or not.\n",
        "    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype=\"bool\") # 145,20\n",
        "    frame_features = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\") #145,20,2048\n",
        "\n",
        "    # For each video.\n",
        "    for idx, path in enumerate(video_paths):\n",
        "        # Gather all its frames and add a batch dimension.\n",
        "        frames = load_video(os.path.join(root_dir, path))\n",
        "        frames = frames[None, ...]\n",
        "\n",
        "        # Initialize placeholders to store the masks and features of the current video.\n",
        "        temp_frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
        "        temp_frame_features = np.zeros(\n",
        "            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
        "        )\n",
        "\n",
        "        # Extract features from the frames of the current video.\n",
        "        for i, batch in enumerate(frames):\n",
        "            video_length = batch.shape[0]\n",
        "            length = min(MAX_SEQ_LENGTH, video_length)\n",
        "            for j in range(length):\n",
        "                temp_frame_features[i, j, :] = feature_extractor.predict(\n",
        "                    batch[None, j, :]\n",
        "                )\n",
        "            temp_frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
        "\n",
        "        frame_features[idx,] = temp_frame_features.squeeze()\n",
        "        frame_masks[idx,] = temp_frame_mask.squeeze()\n",
        "\n",
        "    return (frame_features, frame_masks), labels\n",
        "\n",
        "\n",
        "train_data, train_labels = prepare_all_videos(train_df, \"train\")\n",
        "test_data, test_labels = prepare_all_videos(test_df, \"test\")\n",
        "\n",
        "print(f\"Frame features in train set: {train_data[0].shape}\")\n",
        "print(f\"Frame masks in train set: {train_data[1].shape}\")\n",
        "\n",
        "\n",
        "\n",
        "print(f\"train_labels in train set: {train_labels.shape}\")\n",
        "\n",
        "print(f\"test_labels in train set: {test_labels.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNuLI1hvxTgV"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Utility for our sequence model.\n",
        "def get_sequence_model():\n",
        "    class_vocab = label_processor.get_vocabulary()\n",
        "\n",
        "    frame_features_input = keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))\n",
        "    mask_input = keras.Input((MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
        "\n",
        "    # Refer to the following tutorial to understand the significance of using `mask`:\n",
        "    # https://keras.io/api/layers/recurrent_layers/gru/\n",
        "    x = keras.layers.GRU(16, return_sequences=True)(frame_features_input, mask=mask_input)\n",
        "    x = keras.layers.GRU(8)(x)\n",
        "    x = keras.layers.Dropout(0.4)(x)\n",
        "    x = keras.layers.Dense(8, activation=\"relu\")(x)\n",
        "    output = keras.layers.Dense(len(class_vocab), activation=\"softmax\")(x)\n",
        "\n",
        "    rnn_model = keras.Model([frame_features_input, mask_input], output)\n",
        "\n",
        "    rnn_model.compile(\n",
        "        loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return rnn_model\n",
        "\n",
        "EPOCHS = 30\n",
        "# Utility for running experiments.\n",
        "def run_experiment():\n",
        "    filepath = \"./tmp/video_classifier\"\n",
        "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
        "        filepath, save_weights_only=True, save_best_only=True, verbose=1\n",
        "    )\n",
        "\n",
        "    seq_model = get_sequence_model()\n",
        "    history = seq_model.fit(\n",
        "        [train_data[0], train_data[1]],\n",
        "        train_labels,\n",
        "        validation_split=0.3,\n",
        "        epochs=EPOCHS,\n",
        "        callbacks=[checkpoint],\n",
        "    )\n",
        "\n",
        "    seq_model.load_weights(filepath)\n",
        "    _, accuracy = seq_model.evaluate([test_data[0], test_data[1]], test_labels)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "\n",
        "    return history, seq_model\n",
        "\n",
        "\n",
        "_, sequence_model = run_experiment()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "354jtbYeM4BY"
      },
      "outputs": [],
      "source": [
        "def prepare_single_video(frames):\n",
        "    frames = frames[None, ...]\n",
        "    frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
        "    frame_features = np.zeros(shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\")\n",
        "\n",
        "    for i, batch in enumerate(frames):\n",
        "        video_length = batch.shape[0]\n",
        "        length = min(MAX_SEQ_LENGTH, video_length)\n",
        "        for j in range(length):\n",
        "            frame_features[i, j, :] = feature_extractor.predict(batch[None, j, :])\n",
        "        frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
        "\n",
        "    return frame_features, frame_mask\n",
        "\n",
        "\n",
        "def sequence_prediction(path):\n",
        "    class_vocab = label_processor.get_vocabulary()\n",
        "\n",
        "    frames = load_video(os.path.join(\"test\", path))\n",
        "    frame_features, frame_mask = prepare_single_video(frames)\n",
        "    probabilities = sequence_model.predict([frame_features, frame_mask])[0]\n",
        "\n",
        "    for i in np.argsort(probabilities)[::-1]:\n",
        "        print(f\"  {class_vocab[i]}: {probabilities[i] * 100:5.2f}%\")\n",
        "    return frames\n",
        "\n",
        "test_video = np.random.choice(test_df[\"video_name\"].values.tolist())\n",
        "print(f\"Test video path: {test_video}\")\n",
        "\n",
        "test_frames = sequence_prediction(test_video)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XNnsfaHjM4BZ",
        "outputId": "81affb82-62b4-4daa-c4e0-1b33381e5b76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['2.mp4', '1.mp4']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# folder path\n",
        "dir_path = r'/home/mes2/Desktop/Video Classification/testfile'\n",
        "\n",
        "# list to store files\n",
        "res = []\n",
        "\n",
        "# Iterate directory\n",
        "for path in os.listdir(dir_path):\n",
        "    # check if current path is a file\n",
        "    if os.path.isfile(os.path.join(dir_path, path)):\n",
        "        res.append(path)\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tA90PbiaM4Ba"
      },
      "outputs": [],
      "source": [
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "# import some common libraries\n",
        "import numpy as np\n",
        "import tqdm\n",
        "import cv2\n",
        "# import some common detectron2 utilities\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.video_visualizer import VideoVisualizer\n",
        "from detectron2.utils.visualizer import ColorMode, Visualizer\n",
        "from detectron2.data import MetadataCatalog\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0xwyuTCM4Bb"
      },
      "outputs": [],
      "source": [
        "# Initialize visualizer\n",
        "    #v = VideoVisualizer(MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), ColorMode.IMAGE)\n",
        "def runOnVideo(video, maxFrames):\n",
        "        \"\"\" Runs the predictor on every frame in the video (unless maxFrames is given),\n",
        "        and returns the frame with the predictions drawn.\n",
        "        \"\"\"\n",
        "\n",
        "        readFrames = 0\n",
        "        while True:\n",
        "            hasFrame, frame = video.read()\n",
        "            if not hasFrame:\n",
        "                break\n",
        "\n",
        "            # Get prediction results for this frame\n",
        "            outputs = predictor(frame)\n",
        "\n",
        "            # Make sure the frame is colored\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "            # Draw a visualization of the predictions using the video visualizer\n",
        "            visualization = v.draw_instance_predictions(frame, outputs[\"instances\"].to(\"cpu\"))\n",
        "\n",
        "            # Convert Matplotlib RGB format to OpenCV BGR format\n",
        "            visualization = cv2.cvtColor(visualization.get_image(), cv2.COLOR_RGB2BGR)\n",
        "\n",
        "            yield visualization\n",
        "\n",
        "            readFrames += 1\n",
        "            if readFrames > maxFrames:\n",
        "                break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKUj4cjfM4Bb"
      },
      "outputs": [],
      "source": [
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7  # set threshold for this model\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
        "predictor = DefaultPredictor(cfg)\n",
        "v = VideoVisualizer(MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), ColorMode.IMAGE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLwTU4KsM4Bb"
      },
      "outputs": [],
      "source": [
        "def showPerson():\n",
        "    for txt in res:\n",
        "        # Extract video properties\n",
        "        video = cv2.VideoCapture(dir_path+'/'+txt)\n",
        "        width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "        frames_per_second = video.get(cv2.CAP_PROP_FPS)\n",
        "        num_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "        # Initialize video writer\n",
        "        video_writer = cv2.VideoWriter('out'+txt, fourcc=cv2.VideoWriter_fourcc(*\"mp4v\"), fps=float(frames_per_second), frameSize=(width, height), isColor=True)\n",
        "\n",
        "\n",
        "\n",
        "        # Create a cut-off for debugging\n",
        "        num_frames = 120\n",
        "\n",
        "        # Enumerate the frames of the video\n",
        "        for visualization in tqdm.tqdm(runOnVideo(video, num_frames), total=num_frames):\n",
        "\n",
        "            # Write test image\n",
        "            cv2.imwrite('POSE detectron2.png', visualization)\n",
        "\n",
        "            # Write to video file\n",
        "            video_writer.write(visualization)\n",
        "\n",
        "        # Release resources\n",
        "        video.release()\n",
        "        video_writer.release()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLM2EkrlM4Bb",
        "outputId": "6d83c2c1-1bae-45d4-e7c3-98746307ffad"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "121it [00:28,  4.25it/s]                         \n",
            "121it [00:23,  5.23it/s]                         \n"
          ]
        }
      ],
      "source": [
        "showPerson()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzorMLorM4Bc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "interpreter": {
      "hash": "99e5c381da92035e16fc686a9052d86d02203339bf7ce7f6a8a64c772885aff2"
    },
    "kernelspec": {
      "display_name": "Python 3.7.13 ('micro3')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}